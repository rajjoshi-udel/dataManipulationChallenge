---
title: "Data Manipulation Challenge"
subtitle: "A Mental Model for Method Chaining in Pandas"
format:
  html: default
  pdf: default
execute:
  echo: true
  eval: true
---

# ðŸ”— Data Manipulation Challenge - A Mental Model for Method Chaining in Pandas

## Data Loading and Initial Exploration
## ðŸ¤” Discussion Questions: Assign Mental Model

**Question 1: Data Types and Date Handling**
- What is the `dtype` of the `actualShipDate` series? How can you find out using code?
- Why is it important that both `actualShipDate` and `plannedShipDate` have the same data type for comparison?

**Question 2: String vs Date Comparison**
- Can you give an example where comparing two dates as strings would yield unintuitive results, e.g. what happens if you try to compare "04-11-2025" and "05-20-2024" as strings vs as dates?

```{python}
#| label: load-data
#| echo: true
#| message: false
#| warning: false

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime, timedelta

# Load the shipment data
shipments_df = pd.read_csv(
    "https://raw.githubusercontent.com/flyaflya/persuasive/main/shipments.csv", 
    parse_dates=['plannedShipDate', 'actualShipDate']
)

# Load product line data
product_line_df = pd.read_csv(
    "https://raw.githubusercontent.com/flyaflya/persuasive/main/productLine.csv"
)

# Reduce dataset size for faster processing (4,000 rows instead of 96,805 rows)
shipments_df = shipments_df.head(4000)

print("Shipments data shape:", shipments_df.shape)
print("\nShipments data columns:", shipments_df.columns.tolist())
print("\nFirst few rows of shipments data:")
print(shipments_df.head(10))

print("\n" + "="*50)
print("Product line data shape:", product_line_df.shape)
print("\nProduct line data columns:", product_line_df.columns.tolist())
print("\nFirst few rows of product line data:")
print(product_line_df.head(10))
```

**Question 3: Debug This Code**
```{python}
# This code has an error - can you spot it?
shipments_with_lateness = (
    shipments_df
    .assign(
        is_late=lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late=lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days,
        lateStatement="Darn Shipment is Late" if shipments_df['is_late'] else "Shipment is on Time"
    )
)
```
What's wrong with the `lateStatement` assignment and how would you fix it?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

** Answer to Question 1: Data Types and Date Handling**
```{python}
# Check the dtype of actualShipDate
print(shipments_df['actualShipDate'].dtype)
# Output: datetime64[ns]

# You can also check it using the .dtypes attribute on the dataframe
print(shipments_df.dtypes['actualShipDate'])
```

** Answer to Question 2: String vs Date Comparison**
```{python}
# Example where comparing two dates as strings would yield unintuitive results
print("04-11-2025" > "05-20-2024")  # This would return True because "04-11-2025" is later than "05-20-2024"

# If you convert them to dates first, the comparison works correctly
print(pd.to_datetime("04-11-2025") > pd.to_datetime("05-20-2024"))  # This returns False
```

** Answer to Question 3: Debug This Code**
```{python}
# This code has an error - can you spot it?
shipments_with_lateness = (
    shipments_df
    .assign(
        is_late=lambda df: df['actualShipDate'] > df['plannedShipDate'],
        days_late=lambda df: (df['actualShipDate'] - df['plannedShipDate']).dt.days,
        # Fix the lateStatement assignment by using lambda to reference the current dataframe
        lateStatement=lambda df: df['is_late'].apply(lambda x: "Darn Shipment is Late" if x else  "Shipment is on Time")
    )
)
```
What's wrong with the `lateStatement` assignment and how would you fix it?

### 2. Subset: Querying Rows and Filtering Columns

## ðŸ¤” Discussion Questions: Subset Mental Model

**Question 1: Query vs Boolean Indexing**
- What's the difference between using `.query('is_late == True')` and `[df['is_late'] == True]`?
- Which approach is more readable and why?

**Question 2: Additional Row Querying**
- Can you show an example of using a variable like `late_threshold` to query rows for shipments that are at least `late_threshold` days late, e.g. what if you wanted to query rows for shipments that are at least 5 days late?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

** Answer to Question 1: Query vs Boolean Indexing**
```{python}
# Using .query() - more readable, SQL-like syntax
late_shipments_query = shipments_with_lateness.query('is_late == True')

# Using boolean indexing - more Python-like
late_shipments_boolean = shipments_with_lateness[shipments_with_lateness['is_late'] == True]

# Both produce the same result, but:
# - .query() is more readable for simple conditions
# - .query() allows referencing column names directly as strings
# - Boolean indexing is more Python-like and flexible for complex conditions
# - Boolean indexing can be faster for very large datasets
```

** Answer to Question 2: Additional Row Querying**
```{python}
#| echo: true

# Define the threshold
late_threshold = 5

# REAL-WORLD APPLICATION: Dynamic threshold queries are essential for:
# - Customer service: Quickly identify customers who need proactive communication
# - Operations: Prioritize expedited shipping for critical late orders
# - Finance: Calculate penalty costs for SLA violations (e.g., 5% discount per day late)

# METHOD 1: Using .query() with @ to reference external variable
# WHY THIS MATTERS: Makes queries reusable and maintainable
# BUSINESS VALUE: Operations managers can adjust thresholds without touching complex code
# REAL-WORLD: Data analysts can create dashboard parameters that non-technical users can modify
very_late_query = (
    shipments_with_lateness
    .query('days_late >= @late_threshold')  # @ symbol references variable from outer scope
    .filter(['shipID', 'partID', 'days_late'])  # Keep only relevant columns for reporting
    .sort_values('days_late', ascending=False)  # Worst offenders first for prioritization
)

print("="*70)
print("METHOD 1: Using .query() with @ symbol for variable reference")
print("="*70)
print("BENEFITS: SQL-like readability, easy to understand for analysts coming from SQL backgrounds")
print("USE CASE: When creating dynamic dashboards where thresholds change based on business needs")
print(f"Shipments at least {late_threshold} days late (using .query()):")
print(f"Count: {len(very_late_query)}")
print(very_late_query.head(10))

print("\n" + "="*60)

# METHOD 2: Using boolean indexing (no @ needed)
# WHY THIS MATTERS: More Pythonic, integrates naturally with method chaining
# BUSINESS VALUE: Familiar to data scientists with Python backgrounds, can handle complex conditions
# REAL-WORLD: Better performance for very large datasets, can combine multiple conditions easily
very_late_brackets = (
    shipments_with_lateness[
        shipments_with_lateness['days_late'] >= late_threshold  # Direct boolean indexing
    ]
    [['shipID', 'partID', 'days_late']]  # Select specific columns
    .sort_values('days_late', ascending=False)  # Sort for actionability
)

print("="*70)
print("METHOD 2: Using boolean indexing for row filtering")
print("="*70)
print("BENEFITS: More flexible for complex conditions (e.g., multiple AND/OR conditions)")
print("USE CASE: When building complex filters that combine multiple business rules")
print("REAL-WORLD EXAMPLE: E-commerce sites use this to identify orders requiring expedited shipping")
print(f"Shipments at least {late_threshold} days late (using boolean indexing):")
print(f"Count: {len(very_late_brackets)}")
print(very_late_brackets.head(10))

# COMPREHENSIVE UNDERSTANDING:
print("\n" + "="*70)
print("KEY TAKEAWAYS FOR BUSINESS ANALYSIS:")
print("="*70)
print("1. Variable thresholds enable operational flexibility: Change threshold = change focus")
print("2. Both methods return identical results: Choose based on team's comfort level")
print("3. Sorting by days_late helps prioritize: Focus on worst problems first")
print("4. Real-world impact: Each late shipment may represent lost revenue and customer trust")
```

### 3. Drop: Removing Unwanted Data

## ðŸ¤” Discussion Questions: Drop Mental Model

**Question 1: Drop vs Filter Strategies**
- What's the difference between `.drop(columns=['quantity'])` and `.filter()` with a list of columns you want to keep?
- When would you choose to drop columns vs filter to keep specific columns?

**Question 2: Handling Missing Data**
- What happens if you use `.dropna()` without specifying `subset`? How is this different from `.dropna(subset=['plannedShipDate', 'actualShipDate'])`?
- Why might you want to be selective about which columns to check for missing values?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

** Answer to Question 1: Drop vs Filter Strategies**
```{python}
#| echo: true

print("="*70)
print("COMPREHENSIVE COMPARISON: .drop() vs .filter()")
print("="*70)

# Example dataset with 7 columns
print("\n1. ORIGINAL COLUMNS:", shipments_with_lateness.columns.tolist())
print("   Total columns:", len(shipments_with_lateness.columns))

# KEY DIFFERENCE #1: Intent and Use Case
print("\n" + "="*70)
print("KEY DIFFERENCE #1: INTENT AND USE CASE")
print("="*70)

# METHOD 1: Using .drop() - "Remove what I don't want"
# EXAMPLE: GDPR compliance - remove personal data columns
dropped_columns = (
    shipments_with_lateness
    .drop(columns=['quantity'])  # We know we don't want quantity
)
print(".drop() - Remove unwanted columns")
print("  Use when: You have a dataset with 20 columns and want to remove 2-3 specific ones")
print("  Real-world: Removing PII (Personally Identifiable Information) from customer data")
print("  Example: Drop 'ssn', 'credit_card' from customer database for privacy compliance")
print("  After .drop(columns=['quantity']):", dropped_columns.columns.tolist())

# METHOD 2: Using .filter() - "Keep only what I want"
# EXAMPLE: Executive dashboard - show only key metrics
filtered_columns = (
    shipments_with_lateness
    .filter(['shipID', 'partID', 'is_late', 'days_late'])  # We know exactly what we want
)
print("\n.filter() - Keep only desired columns")
print("  Use when: You have 20 columns but only need 3-4 specific ones")
print("  Real-world: Creating executive dashboards with only KPIs (Key Performance Indicators)")
print("  Example: Keep only 'revenue', 'profit', 'orders' for C-suite reporting")
print("  After .filter(['shipID', 'partID', 'is_late', 'days_late']):", filtered_columns.columns.tolist())

# KEY DIFFERENCE #2: Error Handling
print("\n" + "="*70)
print("KEY DIFFERENCE #2: ERROR HANDLING")
print("="*70)

# .drop() with error='ignore' vs .filter() with regex
print(".drop() behavior:")
print("  - If column doesn't exist, raises KeyError by default")
print("  - Use .drop(columns=['xxx'], errors='ignore') to silently skip missing columns")
print("  - Example: df.drop(columns=['column1', 'column2']) # Both must exist")

print("\n.filter() behavior:")
print("  - If column doesn't exist, returns empty DataFrame")
print("  - Supports regex patterns: .filter(regex='^ship') matches all columns starting with 'ship'")
print("  - Example: df.filter(regex='sales|revenue') matches any column containing 'sales' or 'revenue'")

# KEY DIFFERENCE #3: Performance and Readability
print("\n" + "="*70)
print("KEY DIFFERENCE #3: PERFORMANCE AND READABILITY")
print("="*70)

# Performance consideration
print(".drop() performance:")
print("  - Faster when removing few columns from many (e.g., remove 2 from 100)")
print("  - Requires iterating through list of columns to remove")

print("\n.filter() performance:")
print("  - Faster when keeping few columns from many (e.g., keep 3 from 100)")
print("  - More efficient memory allocation (only copies desired columns)")

# Readability consideration
print("\nReadability:")
print("  - .drop() reads as 'remove these unwanted items'")
print("  - .filter() reads as 'focus on these specific items'")
print("  - Choose based on which intent is clearer for your team")

# DECISION MATRIX
print("\n" + "="*70)
print("DECISION MATRIX: WHEN TO USE WHICH?")
print("="*70)
print("Remove < 30% of columns -> Use .drop()")
print("Keep < 30% of columns -> Use .filter()")
print("Equal number of keep/remove -> Use .filter() (more explicit about intent)")
print("\nREAL-WORLD SCENARIO:")
print("E-commerce: 50 columns in database, dashboard needs 5 KPI columns")
print("  â†’ Use .filter(['revenue', 'orders', 'conversion', 'churn', 'satisfaction'])")
print("  â†’ Much clearer than .drop() with 45 column names!")
```

### 4. Sort: Arranging Data

## ðŸ¤” Discussion Questions: Sort Mental Model

**Question 1: Sorting Strategies**
- What's the difference between `ascending=False` and `ascending=True` in sorting?
- How would you sort by multiple columns (e.g., first by `is_late`, then by `days_late`)?

**Question 2: Index Management**
- Why do we use `.reset_index(drop=True)` after sorting?
- What happens to the original index when you sort? Why might this be problematic?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

** Answer to Question 1: Sorting Strategies**
```{python}
#| echo: true

print("="*70)
print("SORTING STRATEGIES: Ascending vs Descending")
print("="*70)

# Business Context: Sorting determines action priorities
# Ascending = smallest to largest (worst problems first if sorting negative values)
# Descending = largest to smallest (worst problems first if sorting positive values)

# DEMONSTRATION 1: Ascending Order (ascending=True)
print("\n" + "="*70)
print("ASCENDING ORDER (ascending=True)")
print("="*70)
print("Purpose: Smallest values appear first")
print("Business Use: When you want to see minimum or earliest values first")

sorted_ascending = (
    clean_shipments
    .sort_values('days_late', ascending=True)  # Smallest values first
    .reset_index(drop=True)
    .head(10)  # Top 10
)

print("\nFirst 10 shipments (smallest days_late to largest):")
print(sorted_ascending[['shipID', 'partID', 'days_late', 'is_late']])
print("\nInterpretation: On-time or early shipments appear first")

# DEMONSTRATION 2: Descending Order (ascending=False)
print("\n" + "="*70)
print("DESCENDING ORDER (ascending=False)")
print("="*70)
print("Purpose: Largest values appear first")
print("Business Use: Prioritize worst problems first - CRITICAL FOR OPERATIONS")

sorted_descending = (
    clean_shipments
    .sort_values('days_late', ascending=False)  # Largest values first
    .reset_index(drop=True)
    .head(10)  # Top 10
)

print("\nFirst 10 shipments (largest days_late to smallest):")
print(sorted_descending[['shipID', 'partID', 'days_late', 'is_late']])
print("\nInterpretation: Most delayed shipments appear first - ACTIONABLE INSIGHT")

# REAL-WORLD APPLICATION
print("\n" + "="*70)
print("REAL-WORLD APPLICATION: ZappTech Operations Dashboard")
print("="*70)

# Scenario 1: Warehouse needs to expedite shipping for most delayed orders
worst_delays = (
    clean_shipments
    .sort_values('days_late', ascending=False)  # Worst first
    .head(20)  # Top 20 worst cases
)
print("\nðŸš¨ EXPEDITE THESE SHIPMENTS (Most Delayed First):")
print(worst_delays[['shipID', 'days_late']].to_string(index=False))
print(f"\nBusiness Impact: These {len(worst_delays)} shipments need immediate attention")
print("Management Action: Expedite shipping to prevent customer complaints")

# Scenario 2: Customer Service needs to proactively contact customers
print("\nðŸ’¬ PROACTIVE CUSTOMER COMMUNICATION:")
print("Priority customers to contact (sorted by delay severity):")
customer_priority = (
    clean_shipments
    .query('is_late == True')  # Only late shipments
    .sort_values('days_late', ascending=False)  # Worst first
    .head(10)
)
print(customer_priority[['shipID', 'days_late']].to_string(index=False))

# VISUAL COMPARISON
print("\n" + "="*70)
print("VISUAL COMPARISON")
print("="*70)

# Create a comparison DataFrame
comparison = clean_shipments.nlargest(5, 'days_late')[['shipID', 'days_late']].copy()
comparison['rank_ascending'] = range(1, 6)
comparison['rank_descending'] = range(5, 0, -1)
comparison = comparison.sort_values('rank_ascending')

print("\nSame data, different sorting:")
print("ascending=False:", comparison.sort_values('days_late', ascending=False)['days_late'].tolist())
print("ascending=True: ", comparison.sort_values('days_late', ascending=True)['days_late'].tolist())

**# How would you sort by multiple columns (e.g., first by `is_late`, then by `days_late`)?**

# MULTI-COLUMN SORTING
print("\n" + "="*70)
print("MULTI-COLUMN SORTING: Sorting by Multiple Criteria")
print("="*70)
print("Purpose: Create hierarchical sorting (first by is_late, then by days_late)")
print("Business Use: Prioritize all late shipments, then sub-sort by severity")

# Sort first by is_late (False first, True second), then by days_late within each group
multi_sorted = (
    clean_shipments
    .sort_values(['is_late', 'days_late'], ascending=[True, False])
    # ascending=[True, False] means:
    # - is_late: True (False values first, then True)  
    # - days_late: False (largest values first)
    .reset_index(drop=True)
    .head(30)  # Show top 30
)

print("\nðŸ“Š SORTING LOGIC EXPLAINED:")
print("1. First sort by is_late: On-time shipments (False) appear before late ones (True)")
print("2. Within each group, sort by days_late: Most delayed appear first")
print("\nResult: Late shipments bubble to top, with worst delays first")
print(multi_sorted[['shipID', 'is_late', 'days_late']].to_string(index=False))

# VERIFICATION: Show the pattern
print("\n" + "="*70)
print("VERIFICATION: Pattern Check")
print("="*70)
print("Group 1: On-time shipments (is_late=False)")
on_time_group = multi_sorted[multi_sorted['is_late'] == False]
print(f"Count: {len(on_time_group)}")
if len(on_time_group) > 0:
    print(f"  Days late range: {on_time_group['days_late'].min()} to {on_time_group['days_late'].max()}")

print("\nGroup 2: Late shipments (is_late=True)")
late_group = multi_sorted[multi_sorted['is_late'] == True]
print(f"Count: {len(late_group)}")
if len(late_group) > 0:
    print(f"  Days late range: {late_group['days_late'].min()} to {late_group['days_late'].max()}")
    print(f"  Worst offender: {late_group['days_late'].max()} days late")

# REAL-WORLD APPLICATION
print("\n" + "="*70)
print("REAL-WORLD APPLICATION: Operational Dashboard")
print("="*70)
print("Use Case: Customer service queue prioritization")
print("\nQueue Order:")
print("1. All late shipments first (most urgent)")
print("2. Within late shipments, most delayed first")
print("3. On-time shipments processed after urgent cases")

priority_queue = (
    clean_shipments
    .sort_values(['is_late', 'days_late'], ascending=[True, False])
    .assign(
        queue_priority=lambda df: df.groupby('is_late').cumcount() + 1
    )
    .head(15)
)

print("\nðŸ”„ PRIORITY QUEUE (Top 15):")
print(priority_queue[['shipID', 'is_late', 'days_late', 'queue_priority']].to_string(index=False))

print("\n" + "="*70)
print("KEY TAKEAWAY FOR ZappTech MANAGEMENT")
print("="*70)
print("Multi-column sorting enables:")
print("  âœ“ Hierarchical prioritization (urgent first, then by severity)")
print("  âœ“ Clear operational workflows (process worst problems first)")
print("  âœ“ Systematic resource allocation (focus on highest-impact issues)")
print("\nSyntax: .sort_values([col1, col2], ascending=[True, False])")
print("  - Each column gets its own ascending/descending order")
print("  - First column determines primary grouping")
print("  - Subsequent columns provide sub-sorting within groups")
```

** Answer to Question 2: Index Management**

**What happens to the original index when you sort? Why might this be problematic?**

When you sort a DataFrame, **the original index values remain attached to their rows**. This means:
- Row 0 in the sorted data might have been row 15 in the original data
- The index doesn't reflect the new position after sorting
- This creates confusion when accessing by position

**Why it's problematic:**
1. Position-based access fails (`.iloc[0]` isn't actually the first row after sorting)
2. Iterative operations break (loops don't work as expected)
3. Reports and dashboards show incorrect row numbers
4. Merges and joins can produce unexpected results

**Solution:** Always use `.reset_index(drop=True)` after sorting to renumber rows sequentially (0, 1, 2, 3...) and drop the old index.

```{python}
#| echo: true

print("="*70)
print("UNDERSTANDING INDEX MANAGEMENT AFTER SORTING")
print("="*70)

# Create a subset for demonstration
sample_data = clean_shipments.head(20).copy()

print("\n1. ORIGINAL INDEX (Before Sorting):")
print("Sample of original data with original index:")
print(sample_data[['shipID', 'days_late']])

# Sort WITHOUT reset_index
print("\n" + "="*70)
print("2. PROBLEM: Index Remains Unchanged After Sorting")
print("="*70)

sorted_no_reset = sample_data.sort_values('days_late', ascending=False)
print("After sorting by days_late (descending):")
print(sorted_no_reset[['shipID', 'days_late']])

print("\nâš ï¸ NOTICE: The index (0-19) doesn't match the new row order!")
print("Row 1 now shows shipID at original index 15, not index 1")
print("This creates confusion when accessing by position")

# Sort WITH reset_index
print("\n" + "="*70)
print("3. SOLUTION: reset_index(drop=True) After Sorting")
print("="*70)

sorted_with_reset = (
    sample_data
    .sort_values('days_late', ascending=False)
    .reset_index(drop=True)  # Reset to 0, 1, 2, 3... and drop the old index
)

print("After sorting + reset_index(drop=True):")
print(sorted_with_reset[['shipID', 'days_late']])

print("\nâœ… NOTICE: Index now correctly reflects row position!")
print("Row 1 is actually the 2nd row in the sorted dataframe")

# REAL-WORLD PROBLEM
print("\n" + "="*70)
print("4. REAL-WORLD PROBLEM: When Index Mismatch Causes Errors")
print("="*70)

print("Scenario: Warehouse picks top 10 worst delays for expedited shipping")

# WITHOUT reset_index - PROBLEM
worst_without_reset = clean_shipments.sort_values('days_late', ascending=False).head(10)
print("\nâŒ WITHOUT reset_index:")
print("Row 0 might not be the worst (it's from original index)")
print(f"First row index: {worst_without_reset.index[0]}")  # Could be any original index!

# WITH reset_index - SOLUTION
worst_with_reset = (
    clean_shipments
    .sort_values('days_late', ascending=False)
    .reset_index(drop=True)
    .head(10)
)

print("\nâœ… WITH reset_index(drop=True):")
print("Row 0 is guaranteed to be the worst delay")
print(f"First row index: {worst_with_reset.index[0]}")  # Always 0

# DEMONSTRATION: What happens to the index?
print("\n" + "="*70)
print("5. WHAT reset_index(drop=True) ACTUALLY DOES")
print("="*70)

small_sample = clean_shipments.head(5)
print("Original:")
print(small_sample[['shipID', 'days_late']])

print("\nAfter sorting (index preserved from original):")
sorted_sample = small_sample.sort_values('days_late', ascending=False)
print(sorted_sample[['shipID', 'days_late']])

print("\nAfter sorting + reset_index(drop=True) (index reset to 0-4):")
reset_sample = sorted_sample.reset_index(drop=True)
print(reset_sample[['shipID', 'days_late']])

# BUSINESS IMPACT
print("\n" + "="*70)
print("KEY TAKEAWAY FOR ZappTech MANAGEMENT")
print("="*70)
print("Why reset_index(drop=True) matters:")
print("  âœ“ Prevents index confusion (row 0 is actually first row)")
print("  âœ“ Makes iterative operations reliable (for i in range(10) works correctly)")
print("  âœ“ Enables safe position-based access (.iloc[0:5] gets actual first 5 rows)")
print("  âœ“ Critical for reports and dashboards that assume sequential index")
print("\ndrop=True parameter:")
print("  - drop=True: Discards old index (no new column created)")
print("  - drop=False: Old index becomes a new column named 'index'")
print("\nBest Practice: Always use reset_index(drop=True) after sorting")
print("  UNLESS you specifically need to track original row positions")
```

### 5. Aggregate: Summarizing Data

## ðŸ¤” Discussion Questions: Aggregate Mental Model

**Question 1: Boolean Aggregation**
- Why does `sum()` work on boolean values? What does it count?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

** Answer to Question 1: Boolean Aggregation**

```{python}
#| echo: true

#clean_shipments.agg({
#        'is_late': ['count', 'sum', 'mean']
#    })

print(f"Total shipments: {clean_shipments['is_late'].sum()}")

print(f"Total late shipments: {clean_shipments.query('is_late == True').filter(['is_late']).count()}")
```
`sum()` works on boolean values because it counts the number of True values in the series. See above example using sum and query for late shipments's count. Both are same result.


### 6. Merge: Combining Information

## ðŸ¤” Discussion Questions: Merge Mental Model

**Question 1: Join Types and Data Loss**
- Why does your professor think we should use `how='left'` in most cases? 
- How can you check if any shipments were lost during the merge?

**Question 2: Key Column Matching**
- What happens if there are duplicate `partID` values in the `product_line_df`?

:::

#### Briefly Give Answers to the Discussion Questions In This Section

** Answer to Question 1: Join Types and Data Loss**

Using `how='left'` in most cases is because we want to keep all the data from the left dataframe and only add the data from the right dataframe if there is a match. If there is no match, the result will be `NaN`.
```{python}
# Compare different join types
print("Understanding Join Types:\n")

# Check sizes before merge
print(f"clean_shipments rows: {len(clean_shipments)}")
print(f"product_line_df rows: {len(product_line_df)}")
print()

# LEFT JOIN - Keep ALL shipments
left_join = clean_shipments.merge(product_line_df, on='partID', how='left')
print(f"LEFT JOIN:  {len(left_join)} rows (kept all shipments âœ“)")

# INNER JOIN - Only matching rows
inner_join = clean_shipments.merge(product_line_df, on='partID', how='inner')
print(f"INNER JOIN: {len(inner_join)} rows (lost {len(clean_shipments) - len(inner_join)} shipments âœ—)")
```

**Why LEFT JOIN is better than INNER JOIN?**
LEFT JOIN is better than INNER JOIN because it keeps all the data from the left dataframe and only adds the data from the right dataframe if there is a match. If there is no match, the result will be `NaN`.
```{python}
# Compare different join types
print("Understanding Join Types:\n")

# Check sizes before merge
print(f"clean_shipments rows: {len(clean_shipments)}")
print(f"product_line_df rows: {len(product_line_df)}")
print()
```

** Answer to Question 2: Key Column Matching**
```{python}
#| echo: true

# Check for duplicates in the product_line_df
print("Checking for duplicate partIDs in product_line_df:\n")
duplicate_count = product_line_df['partID'].duplicated().sum()
print(f"Duplicate partIDs: {duplicate_count}")

if duplicate_count > 0:
    print("\nâš ï¸  WARNING: Duplicates found!")
    print("This will create multiple rows for each shipment!")
    
    # Show an example
    dup_parts = product_line_df[product_line_df['partID'].duplicated(keep=False)].sort_values('partID')
    print("\nExample duplicates:")
    print(dup_parts.head(10))
else:
    print("âœ“ No duplicates - each partID is unique")
    print("This is good: 1-to-1 mapping between parts and categories")
```

**What happens with duplicates:**

If `product_line_df` has duplicate `partID` values, the merge will create **multiple rows** for each shipment:

```{python}
# Example:
clean_shipments:         product_line_df:        Result after merge:
shipID  partID          partID  category         shipID  partID  category
10001   A123            A123    Electronics      10001   A123    Electronics
                        A123    Accessories      10001   A123    Accessories  â† Duplicate!
```

**Result:** 1 shipment becomes 2 rows! This inflates your data and ruins aggregations.

**How to prevent:**
```{python}
# Remove duplicates before merge
product_line_clean = product_line_df.drop_duplicates(subset=['partID'])

# Or check first
assert product_line_df['partID'].is_unique, "Duplicates found!"
```
### 7. Split-Apply-Combine: Group Analysis

## ðŸ¤” Discussion Questions: Split-Apply-Combine Mental Model

**Question 1: GroupBy Mechanics**
- What does `.groupby('prodCategory')` actually do? How does it "split" the data?
- Why do we need to use `.agg()` after grouping? What happens if you don't?

**Question 2: Multi-Level Grouping**
- Explore grouping by `['shipID', 'prodCategory']`?  What question does this answer versus grouping by `'prodCategory'` alone?  (HINT: There may be many rows with identical shipID's due to a particular order having multiple partID's.)
:::

#### Briefly Give Answers to the Discussion Questions In This Section

** Answer to Question 1: GroupBy Mechanics**
**What does `.groupby('prodCategory')` actually do? How does it "split" the data?**

`.groupby()` divides your DataFrame into separate groups based on unique values in the specified column(s). see below example.

```{python}
#| echo: true

# Let's see what groupby does step by step
print("Understanding GroupBy:\n")

# First, see what categories we have
print("Unique product categories:")
print(shipments_with_category['prodCategory'].value_counts())
print()

# When we groupby, it creates separate groups for each category
grouped = shipments_with_category.groupby('prodCategory')
print(f"Type of grouped object: {type(grouped)}")
print(f"Number of groups: {grouped.ngroups}")
print(f"Group names (keys): {list(grouped.groups.keys())}")
print()

# Look at one group
print("="*60)
print("Example: Looking at the 'Liquids' group")
liquids_group = grouped.get_group('Liquids')
print(f"Rows in Liquids group: {len(liquids_group)}")
print(liquids_group[['shipID', 'partID', 'prodCategory', 'is_late']].head())
```

**How it "splits" the data:**

1. **Identifies unique values** in the groupby column
2. **Separates rows** into groups based on those values
3. **Creates a GroupBy object** (not a DataFrame yet!)

```{python}
#| echo: true

# Visual demonstration of the split
print("\nVisual representation of how data is split:\n")

for category, group in grouped:
    print(f"{category}: {len(group)} rows")
    print(f"  Late shipments: {group['is_late'].sum()}")
    print(f"  On-time rate: {(1 - group['is_late'].mean()) * 100:.1f}%")
    print()
```

**Think of it like this:**
```
Original DataFrame:          After .groupby('prodCategory'):
shipID  prodCategory         
10001   Liquids              Group 1: Liquids
10002   Machines             â”œâ”€ All rows with 'Liquids'
10003   Liquids              
10004   Machines             Group 2: Machines
10005   Liquids              â”œâ”€ All rows with 'Machines'
                             
                             Group 3: Marketables
                             â”œâ”€ All rows with 'Marketables'
                             
                             Group 4: SpareParts
                             â”œâ”€ All rows with 'SpareParts'
```

**Why do we need to use `.agg()` after grouping? What happens if you don't?**

`.groupby()` alone just organizes the data - it doesn't calculate anything! You need `.agg()` to tell it WHAT to calculate for each group.

```{python}
#| echo: true

print("Understanding why we need .agg():\n")

# Step 1: Just groupby - creates a GroupBy object
just_grouped = shipments_with_category.groupby('prodCategory')
print(f"1. Just .groupby(): {type(just_grouped)}")
print("   This is NOT a DataFrame - it's a GroupBy object")
print("   You can't display it or use it directly!")
print()

# Step 2: Try to print it (won't show data)
print(f"2. Trying to use just_grouped directly:")
print(f"   {just_grouped}")
print("   See? Just shows it's a GroupBy object, not the data!")
print()

# Step 3: Use .agg() to get actual results
print(f"3. Using .agg() to calculate statistics:")
with_agg = just_grouped.agg({
    'is_late': ['count', 'sum', 'mean']
})
print(with_agg)
print("\n   NOW we have a DataFrame with actual results!")
```

**What happens if you don't use `.agg()`?**

```{python}
#| echo: true

print("\nOptions after .groupby():\n")

grouped = shipments_with_category.groupby('prodCategory')

# Option 1: Use specific aggregation methods directly
print("Option 1: Direct aggregation methods")
print(grouped['is_late'].sum())  # Works! Shortcut for .agg('sum')
print()

# Option 2: Use .agg() for multiple calculations
print("Option 2: Use .agg() for complex aggregations")
result = grouped.agg({
    'is_late': ['count', 'sum', 'mean'],
    'days_late': ['mean', 'max']
})
print(result)
print()

# Option 3: Try to use grouped object without aggregation
print("Option 3: Try to print grouped object without aggregation")
try:
    print(grouped)  # Just shows the object type
except Exception as e:
    print(f"Error: {e}")
```

**Key Insights:**

1. **`.groupby()` creates groups** but doesn't calculate anything
2. **`.agg()` tells pandas WHAT to calculate** for each group
3. **Without `.agg()`**, you have a GroupBy object (not usable data)
4. **With `.agg()`**, you get a DataFrame with results



**Common aggregation functions:**
```{python}
.agg('sum')      # Total
.agg('mean')     # Average
.agg('count')    # Count rows
.agg('min')      # Minimum
.agg('max')      # Maximum
.agg(['sum', 'mean', 'count'])  # Multiple at once!
```

** Answer to Question 2: Key Column Matching**

**What does grouping by `['shipID', 'prodCategory']` do versus just `'prodCategory'`?**

```{python}
#| echo: true

print("Comparing single vs multi-level grouping:\n")

# Single-level: Group by prodCategory only
single_group = (
    shipments_with_category
    .groupby('prodCategory')
    .agg({
        'is_late': ['count', 'sum']
    })
)
print("Single-level grouping by 'prodCategory':")
print(single_group)
print("\nQuestion answered: How many shipments per category are late?")
print()

print("="*60)

# Multi-level: Group by BOTH shipID and prodCategory
multi_group = (
    shipments_with_category
    .groupby(['shipID', 'prodCategory'])
    .agg({
        'is_late': 'any',  # True if ANY item late
        'partID': 'count'   # Count items in this shipment/category combo
    })
    .reset_index()
)
print("\nMulti-level grouping by ['shipID', 'prodCategory']:")
print(multi_group.head(15))
print("\nQuestion answered: For EACH shipment, what categories does it contain?")
```

**Key Difference:**

**Single-level `['prodCategory']`:**
- Groups ALL shipments by category
- Answer: "How do categories perform overall?"

**Multi-level `['shipID', 'prodCategory']`:**
- Groups by BOTH shipID AND category
- Creates one row per unique combination
- Answer: "Which categories are in each specific shipment?"

**Why is this useful?**
```{python}
#| echo: true

print("\nWhy multi-level grouping matters:\n")

# Use multi-level to find shipments with multiple categories
multi_category_info = (
    shipments_with_category
    .groupby('shipID')
    .agg({
        'prodCategory': 'nunique',  # Count unique categories
        'partID': 'count'           # Count total items
    })
    .reset_index()
)

# Find shipments with multiple categories
multi_cat = multi_category_info[multi_category_info['prodCategory'] > 1]
print(f"Shipments with multiple categories: {len(multi_cat)}")
print(f"Percentage: {len(multi_cat) / len(multi_category_info) * 100:.1f}%")
print("\nExamples:")
print(multi_cat.head(10))
```

**Business Insight:**
- Single-level: Category performance
- Multi-level: Cross-category shopping behavior!

## Answering A Business Question

**Mental Model:** Combine multiple data manipulation techniques to answer complex business questions.

Let's create a comprehensive analysis by combining shipment-level data with category information:

```{python}
#| label: mental-model-7-comprehensive
#| echo: true

# Create a comprehensive analysis dataset
comprehensive_analysis = (
    shipments_with_category
    .groupby(['shipID', 'prodCategory'])  # Group by shipment and category
    .agg({
        'is_late': 'any',  # True if any item in this shipment/category is late
        'days_late': 'max'  # Maximum days late for this shipment/category
    })
    .reset_index()
    .assign(
        has_multiple_categories=lambda df: df.groupby('shipID')['prodCategory'].transform('nunique') > 1
    )
)

print("Comprehensive analysis - shipments with multiple categories:")
multi_category_shipments = comprehensive_analysis[comprehensive_analysis['has_multiple_categories']]
print(f"Shipments with multiple categories: {multi_category_shipments['shipID'].nunique()}")
print(f"Total unique shipments: {comprehensive_analysis['shipID'].nunique()}")
print(f"Percentage with multiple categories: {multi_category_shipments['shipID'].nunique() / comprehensive_analysis['shipID'].nunique() * 100:.1f}%")
```

::: {.callout-important}
## ðŸ¤” Discussion Questions: Answering A Business Question

**Question 1: Business Question Analysis**
- What business question does this comprehensive analysis answer?
- How does grouping by `['shipID', 'prodCategory']` differ from grouping by just `'prodCategory'`?
- What insights can ZappTech's management gain from knowing the percentage of multi-category shipments?
:::

#### Briefly Give Answers to the Discussion Questions In This Section

**What business question does this comprehensive analysis answer?**

The comprehensive analysis answers: **"Do customers order products from multiple categories in a single shipment, and how common is this cross-category shopping behavior?"**

```{python}
#| echo: true

print("Understanding the Business Question:\n")

# The comprehensive analysis groups by BOTH shipID and prodCategory
print("Step 1: Group by shipID and prodCategory")
print(comprehensive_analysis[['shipID', 'prodCategory', 'is_late', 'has_multiple_categories']].head(5))
print()

# Key metric: How many shipments have multiple categories?
multi_cat_shipments = comprehensive_analysis[comprehensive_analysis['has_multiple_categories']]
total_shipments = comprehensive_analysis['shipID'].nunique()
multi_cat_count = multi_cat_shipments['shipID'].nunique()

print("="*60)
print("BUSINESS QUESTION ANSWERED:")
print("="*60)
print(f"Total unique shipments: {total_shipments}")
print(f"Shipments with multiple categories: {multi_cat_count}")
print(f"Percentage: {multi_cat_count / total_shipments * 100:.1f}%")
print()

# Show examples of multi-category shipments
print("Examples of multi-category shipments:")
sample_multi = comprehensive_analysis[comprehensive_analysis['has_multiple_categories']].head(10)
print(sample_multi[['shipID', 'prodCategory', 'is_late']])
```

**This answers critical business questions:**
1. Are customers buying across product categories?
2. How common is cross-category shopping?
3. Do multi-category orders have different service levels i.e. Expedite service level?

**How does grouping by `['shipID', 'prodCategory']` differ from grouping by just `'prodCategory'`?**

```{python}
#| echo: true

print("\nComparing Single vs Multi-Column Grouping:\n")

# Method 1: Group by 'prodCategory' only
print("Method 1: Grouping by 'prodCategory' only")
category_only = (
    shipments_with_category
    .groupby('prodCategory')
    .agg({
        'is_late': ['count', 'sum', 'mean']
    })
)
print(category_only)
print("\nðŸ“Š Answers: 'How many shipments are late per category?'")
print("   Focus: Category-level performance")
print(f"   Result: {len(category_only)} rows (one per category)")
print()

print("="*60)

# Method 2: Group by BOTH ['shipID', 'prodCategory']
print("\nMethod 2: Grouping by ['shipID', 'prodCategory']")
shipment_and_category = (
    shipments_with_category
    .groupby(['shipID', 'prodCategory'])
    .agg({
        'is_late': 'any',
        'partID': 'count'
    })
    .reset_index()
    .head(20)
)
print(shipment_and_category)
print("\nðŸ“Š Answers: 'What categories does EACH shipment contain?'")
print("   Focus: Individual shipment composition")
print(f"   Result: {len(comprehensive_analysis)} rows (one per shipment-category combo)")
```

**Key Differences:**

| Aspect | Group by 'prodCategory' | Group by ['shipID', 'prodCategory'] |
|--------|------------------------|-------------------------------------|
| **Granularity** | Category level | Shipment-category level |
| **Question** | "How do categories perform?" | "What's in each shipment?" |
| **Rows** | 4 (one per category) | 1000+ (one per combo) |
| **Use Case** | Category performance | Cross-shopping analysis |

```{python}
#| echo: true

# Demonstrate the difference with a specific example
print("\nConcrete Example:\n")

# Pick one shipment with multiple categories
sample_ship = comprehensive_analysis[
    (comprehensive_analysis['has_multiple_categories']) & 
    (comprehensive_analysis['shipID'] == comprehensive_analysis[comprehensive_analysis['has_multiple_categories']]['shipID'].iloc[0])
]

if len(sample_ship) > 0:
    ship_id = sample_ship['shipID'].iloc[0]
    print(f"Shipment {ship_id} contains:")
    for _, row in sample_ship.iterrows():
        print(f"  - {row['prodCategory']}: {'Late' if row['is_late'] else 'On-time'}")
    print()
    print("Single-level grouping would show:")
    print("  'Liquids: X shipments, Y late'")
    print()
    print("Multi-level grouping shows:")
    print(f"  'Shipment {ship_id} has Liquids AND Machines' (cross-category order!)")
```

**What insights can ZappTech's management gain from knowing the percentage of multi-category shipments?**

```{python}
#| echo: true

print("\nBusiness Insights from Multi-Category Analysis:\n")

# Calculate key metrics
total_unique_shipments = comprehensive_analysis['shipID'].nunique()
multi_category_shipments = comprehensive_analysis[comprehensive_analysis['has_multiple_categories']]['shipID'].nunique()
percentage = multi_category_shipments / total_unique_shipments * 100

print(f"ðŸ“ˆ Key Metric: {percentage:.1f}% of shipments contain multiple categories")
print()

# Compare service levels
multi_cat_late = comprehensive_analysis[
    comprehensive_analysis['has_multiple_categories']
]['is_late'].mean()

single_cat_late = comprehensive_analysis[
    ~comprehensive_analysis['has_multiple_categories']
]['is_late'].mean()

print("Service Level Comparison:")
print(f"  Multi-category orders late rate: {multi_cat_late * 100:.1f}%")
print(f"  Single-category orders late rate: {single_cat_late * 100:.1f}%")
print()

# Calculate revenue impact (assuming multi-category = higher value)
print("="*60)
print("MANAGEMENT INSIGHTS:")
print("="*60)
```

**1. Cross-Selling Opportunity:**
- 23.3% of customers buy from multiple categories
- Opportunity to increase cross-selling to the other 76.7%
- Product bundling strategies could boost average order value

**2. Customer Segmentation:**
- Multi-category buyers are more valuable customers
- Different marketing/retention strategies needed
- Target cross-category promotions to single-category buyers

**3. Warehouse & Logistics Planning:**
- 23% of orders require picking from multiple zones
- May need different fulfillment strategies
- Cross-category orders might have different complexity

**4. Inventory Management:**
- Need to stock complementary products together
- Understand which categories are bought together
- Optimize warehouse layout for common combinations

**5. Service Level Strategy:**
- Compare late rates: Multi-category vs single-category
- May need different SLAs for complex orders
- Resource allocation based on order complexity

**6. Marketing Strategy:**
```{python}
# If 23% buy multi-category naturally:
# - Promote bundles to increase this to 30-35%
# - Study what triggers cross-category purchases
# - Create "Complete the Set" campaigns
```

**7. Revenue Implications:**
```{python}
# Assumptions:
# - Single-category order avg: $100
# - Multi-category order avg: $250 (2.5x)

single_cat_revenue = (total_unique_shipments * 0.767) * 100
multi_cat_revenue = (total_unique_shipments * 0.233) * 250
total_revenue = single_cat_revenue + multi_cat_revenue

# If we increase multi-category from 23% to 30%:
# Additional revenue = 7% of customers Ã— $150 extra = significant!
```

**Action Items for ZappTech:**
1. **Identify which categories are commonly bought together**
2. **Create product bundles** based on shopping patterns
3. **Optimize warehouse layout** for common combinations
4. **Develop targeted marketing** for cross-category promotions
5. **Monitor service levels** by order complexity
6. **Set appropriate expectations** for complex orders


## Student Analysis Section: Mastering Data Manipulation {#student-analysis-section}



